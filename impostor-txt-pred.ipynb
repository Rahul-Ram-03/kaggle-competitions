{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d7b192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home-pc/miniconda3/envs/pytorch-basics/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as N\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics import MultilabelAccuracy\n",
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "from torchinfo import summary\n",
    "from IPython.display import clear_output\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ff7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batchSize = 8\n",
    "dModel = 1024\n",
    "vocabSize = len(tokenizer.get_vocab())\n",
    "nLayers = 8\n",
    "nHeads = 8\n",
    "ffDim = 1024\n",
    "\n",
    "lr = 7e-5\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577f647",
   "metadata": {},
   "source": [
    "## Approach 1: Transformer encoder + linear layers for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_AND_LABELS_M1 = \"./kaggle/input/fake-or-real-the-impostor-hunt/data/train_files_and_ground_truth-m1.csv\"\n",
    "TEST_FILES_M1 = \"./kaggle/input/fake-or-real-the-impostor-hunt/data/test_files_m1.csv\"\n",
    "OUT_FILE_M1 = \"./kaggle/working/fake-or-real-the-impostor-hunt/submission-method-1.csv\"\n",
    "\n",
    "maxSeqLen = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47926ef4",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "\tdef __init__(self, path):\n",
    "\t\tsuper(TrainDataset, self).__init__()\n",
    "\t\tself.__filesAndLabels__: pd.DataFrame = pd.read_csv(path)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.__filesAndLabels__.shape[0]\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tinputPath, groundTruth = self.__filesAndLabels__.iloc[index]\n",
    "\t\tinputText = \"\"\n",
    "\n",
    "\t\twith open(inputPath, 'r') as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tinputText += line + \"\\n\"\n",
    "\t\t\n",
    "\t\treturn inputText, groundTruth\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66348fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "\tdef __init__(self, path):\n",
    "\t\tsuper(TestDataset, self).__init__()\n",
    "\t\tself.__articlesAndFiles__: pd.DataFrame = pd.read_csv(path)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.__articlesAndFiles__.shape[0]\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tarticleId, fileId, inputPath = self.__articlesAndFiles__.iloc[index]\n",
    "\t\tinputText = \"\"\n",
    "\n",
    "\t\twith open(inputPath, 'r') as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tinputText += line + \"\\n\"\n",
    "\t\t\n",
    "\t\treturn articleId, fileId, inputText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18035da",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = TrainDataset(FILES_AND_LABELS_M1)\n",
    "testData = TestDataset(TEST_FILES_M1)\n",
    "\n",
    "trainDataloader = DataLoader(trainData, batch_size= batchSize,\n",
    "\t\t\t\t\t\t\t shuffle= True, drop_last= True)\n",
    "testDataLoader = DataLoader(testData, batch_size= batchSize,\n",
    "\t\t\t\t\t\t\tshuffle= True, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965abb9",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb534e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(N.Module):\n",
    "\tclass PositionalEmbedding(N.Module):\n",
    "\t\tdef __init__(self, dModel):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.dModel = dModel\n",
    "\n",
    "\t\tdef forward(self, input):\n",
    "\t\t\temb = math.log(10000) / (self.dModel // 2 - 1)\n",
    "\t\t\temb = torch.exp(torch.arange(self.dModel // 2) * -emb)\n",
    "\t\t\temb = input[:, None] * emb[None, :]\n",
    "\t\t\temb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\t\t\treturn emb\n",
    "\n",
    "\tdef __init__(self, dModel, maxSeqLen, nLayers, nHeads, ffDim, vocabSize, dropout= 0.1):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tencoder = N.TransformerEncoderLayer(d_model= dModel, nhead= nHeads,\n",
    "\t\t\t\t\t\t\t\t\t  \t\tdim_feedforward= ffDim, dropout= dropout,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_first= True)\n",
    "\t\t\n",
    "\t\tself.embeddings = N.Embedding(num_embeddings= vocabSize, embedding_dim= dModel).to(device)\n",
    "\t\tself.posEmb = self.PositionalEmbedding(dModel= dModel)\n",
    "\t\tself.transformerEncoder = N.TransformerEncoder(encoder_layer= encoder, num_layers= nLayers)\n",
    "\t\tself.fc1 = N.Linear(in_features= dModel, out_features= dModel//2)\n",
    "\t\tself.fc2 = N.Linear(in_features= dModel//2, out_features= 2)\n",
    "\t\tself.fc3 = N.Linear(in_features= maxSeqLen * 2, out_features= 1)\n",
    "\n",
    "\tdef forward(self, input, padding_mask):\n",
    "\t\tembs = self.embeddings(input)\n",
    "\t\tbs, l, h = embs.shape\n",
    "\n",
    "\t\tseqIdx = torch.arange(l)\n",
    "\t\tposEmb = self.posEmb(seqIdx).reshape(1, l, h).expand(bs, l, h).to(device)\n",
    "\t\tembs = embs + posEmb\n",
    "\n",
    "\t\tcausalMask = torch.triu(torch.ones(l, l), 1).bool().to(device)\n",
    "\n",
    "\t\toutput = self.transformerEncoder(src= embs, mask= causalMask, \n",
    "\t\t\t\t\t\t\t\t   \t\t src_key_padding_mask= padding_mask)\n",
    "\n",
    "\t\tif(output.isnan().any()):\n",
    "\t\t\traise ValueError\n",
    "\t\t\n",
    "\t\toutput = self.fc1(output)\n",
    "\t\toutput = self.fc2(output).flatten(1,2)\n",
    "\t\toutput: torch.Tensor = self.fc3(output).squeeze(-1)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d20493",
   "metadata": {},
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3529163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder(dModel, maxSeqLen, nLayers, nHeads, ffDim, vocabSize)\n",
    "model.to(device)\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[\n",
    "        torch.zeros((batchSize, maxSeqLen), dtype= torch.long).to(device),  # input\n",
    "        torch.zeros((batchSize, maxSeqLen), dtype= torch.bool).to(device)   # padding_mask\n",
    "    ],\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "    row_settings=['var_names'],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a2991",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "loss_fn = N.BCEWithLogitsLoss()\n",
    "\n",
    "epochSnapshot: list[dict] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\tclear_output(wait= True)\n",
    "\n",
    "\tprint(f\"Starting epoch {epoch + 1} of {epochs}\\n~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\tepochLoss = 0\n",
    "\tepochAcc = 0\n",
    "\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tgc.collect()\n",
    "\n",
    "\tfor idx, (inputText, groundTruth) in enumerate(trainDataloader):\n",
    "\t\tcurrBatchLoss = 0\n",
    "\t\tcurrBatchAccuracy = 0\n",
    "\t\tgroundTruth = groundTruth.cuda()\n",
    "\n",
    "\t\ttokens = tokenizer(inputText, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\n",
    "\t\ttoken_ids = tokens['input_ids'].to(device)\n",
    "\t\tpadding_mask = (~(tokens['attention_mask'].bool())).to(device)\n",
    "\t\tbs = token_ids.shape[0]\n",
    "        \n",
    "        # Shift the input sequence to create the target sequence\n",
    "\t\ttarget_ids = torch.cat((token_ids[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(bs, 1, device=device).long()), 1)\n",
    "\t\t\n",
    "\t\tpred = model(target_ids, padding_mask)\n",
    "\t\tloss = loss_fn(pred, groundTruth.float())\n",
    "\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\tcurrBatchLoss = loss.detach().item()\n",
    "\t\tcurrBatchAccuracy += ((pred > 0.5).int() == groundTruth).sum()\n",
    "\n",
    "\t\tepochLoss += currBatchLoss\n",
    "\t\tepochAcc += (currBatchAccuracy / 16)\n",
    "\n",
    "\t\tif(idx % 5 == 0):\n",
    "\t\t\tprint(f\"Batch: {idx}; Curr batch loss: {currBatchLoss: 0.5f}; Curr batch acc.: {currBatchAccuracy/16*100:0.2f}\")\n",
    "\t\t\n",
    "\tepochLoss /= len(trainDataloader)\n",
    "\tepochAcc /= len(trainDataloader)\n",
    "\t\n",
    "\tepochSnapshot.append({\n",
    "\t\t'epoch': f\"{epoch}\",\n",
    "\t\t'train_loss': f\"{epochLoss:0.7f}\",\n",
    "\t\t'train_acc': f\"{epochAcc:0.5f}\",\n",
    "\t})\n",
    "\n",
    "\tprint(f\"Avg train loss: {epochLoss:0.5f}; Avg train acc.: {epochAcc:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee34b2e",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b12826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "testingPreds: list[dict] = []\n",
    "with torch.inference_mode():\n",
    "\tfor idx, (articleId, fileId, inputText) in enumerate(testDataLoader):\n",
    "\t\ttokens = tokenizer(inputText, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\n",
    "\t\ttoken_ids = tokens['input_ids'].to(device)\n",
    "\t\tpadding_mask = (~(tokens['attention_mask'].bool())).to(device)\n",
    "\t\tbs = token_ids.shape[0]\n",
    "        \n",
    "        # Shift the input sequence to create the target sequence\n",
    "\t\ttarget_ids = torch.cat((token_ids[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(bs, 1, device=device).long()), 1)\n",
    "\t\t\n",
    "\t\tpreds = model(target_ids, padding_mask)\n",
    "\t\t\n",
    "\t\tfor i in range(batchSize):\n",
    "\t\t\ttestingPreds.append({\n",
    "                \"articleId\": articleId[i].item(),\n",
    "                \"fileId\": fileId[i].item(),\n",
    "                \"prediction\": preds[i].item()\n",
    "\t\t\t})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb046df7",
   "metadata": {},
   "source": [
    "### Prepping the submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(testingPreds)\n",
    "\n",
    "submissions_df = df.loc[df.groupby('articleId')['prediction'].idxmax()].reset_index(drop=True)\n",
    "submissions_df.drop(columns= ['prediction'], inplace= True)\n",
    "submissions_df.rename(columns= {\n",
    "\t'articleId': 'id',\n",
    "\t'fileId': 'real_text_id'\n",
    "}, inplace= True)\n",
    "\n",
    "submissions_df.to_csv(OUT_FILE_M1, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0630e",
   "metadata": {},
   "source": [
    "## Approach 2: Combined embeddings to linear layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862fcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_AND_LABELS_M2 = \"./kaggle/input/fake-or-real-the-impostor-hunt/data/train_files_and_ground_truth-m2.json\"\n",
    "TEST_FILES_M2 = \"./kaggle/input/fake-or-real-the-impostor-hunt/data/test_files_m2.json\"\n",
    "OUT_FILE_M2 = \"./kaggle/working/fake-or-real-the-impostor-hunt/submission-method-2.csv\"\n",
    "SEP_TOKEN_ID = tokenizer.sep_token_id\n",
    "\n",
    "maxSeqLen = tokenizer.model_max_length\n",
    "# +1 because [SEP] token...\n",
    "tokenizer.model_max_length = (tokenizer.model_max_length * 2) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014023f",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTrainDataset(Dataset):\n",
    "\tdef __init__(self, path):\n",
    "\t\tsuper(ModifiedTrainDataset, self).__init__()\n",
    "\t\tself.__filesAndLabels__: pd.DataFrame = pd.read_json(path)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.__filesAndLabels__.shape[0]\n",
    "\t\n",
    "\tdef __getfiletext__(self, filePath: str):\n",
    "\t\tinputText = \"\"\n",
    "\t\twith open(filePath, 'r') as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tinputText += line + \"\\n\"\n",
    "\n",
    "\t\treturn inputText\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t_, files = self.__filesAndLabels__.iloc[index]\n",
    "\t\tfiles = sorted(files, key= lambda x: x['fileId'])\n",
    "\n",
    "\t\tfile1_path = files[0]['filePath']\n",
    "\t\tfile1_label = int(files[0]['label'])\n",
    "\t\tfile1_text = self.__getfiletext__(file1_path)\n",
    "\n",
    "\t\tfile2_path = files[1]['filePath']\n",
    "\t\tfile2_label = int(files[1]['label'])\n",
    "\t\tfile2_text = self.__getfiletext__(file2_path)\n",
    "\n",
    "\t\tlabel_tensor = torch.tensor([file1_label, file2_label], dtype= torch.float)\n",
    "\n",
    "\t\treturn file1_text, file2_text, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49175b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTestDataset(Dataset):\n",
    "\tdef __init__(self, path):\n",
    "\t\tsuper(ModifiedTestDataset, self).__init__()\n",
    "\t\tself.__articleAndFiles__: pd.DataFrame = pd.read_json(path)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.__articleAndFiles__.shape[0]\n",
    "\t\n",
    "\tdef __getfiletext__(self, filePath: str):\n",
    "\t\tinputText = \"\"\n",
    "\t\twith open(filePath, 'r') as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tinputText += line + \"\\n\"\n",
    "\n",
    "\t\treturn inputText\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tarticleId, files = self.__articleAndFiles__.iloc[index]\n",
    "\t\tfiles = sorted(files, key= lambda x: x['fileId'])\n",
    "\n",
    "\t\tfile1_path = files[0]['filePath']\n",
    "\t\tfile1_text = self.__getfiletext__(file1_path)\n",
    "\n",
    "\t\tfile2_path = files[1]['filePath']\n",
    "\t\tfile2_text = self.__getfiletext__(file2_path)\n",
    "\n",
    "\t\treturn articleId.item(), file1_text, file2_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b88f2f",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aa54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiedTrainData = ModifiedTrainDataset(FILES_AND_LABELS_M2)\n",
    "modifiedTestData = ModifiedTestDataset(TEST_FILES_M2)\n",
    "\n",
    "modTrainDataloader = DataLoader(modifiedTrainData, batch_size= batchSize,\n",
    "\t\t\t\t\t\t\t shuffle= True, drop_last= True)\n",
    "modTestDataLoader = DataLoader(modifiedTestData, batch_size= 4,\n",
    "\t\t\t\t\t\t\tshuffle= True, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68ae7a",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(N.Module):\n",
    "\tclass PositionalEmbedding(N.Module):\n",
    "\t\tdef __init__(self, dModel):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.dModel = dModel\n",
    "\n",
    "\t\tdef forward(self, input):\n",
    "\t\t\temb = math.log(10000) / (self.dModel // 2 - 1)\n",
    "\t\t\temb = torch.exp(torch.arange(self.dModel // 2) * -emb)\n",
    "\t\t\temb = input[:, None] * emb[None, :]\n",
    "\t\t\temb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\t\t\treturn emb\n",
    "\n",
    "\tdef __init__(self, dModel, maxSeqLen, nLayers, nHeads, ffDim, vocabSize, dropout= 0.1):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tencoder = N.TransformerEncoderLayer(d_model= dModel, nhead= nHeads,\n",
    "\t\t\t\t\t\t\t\t\t  \t\tdim_feedforward= ffDim, dropout= dropout,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_first= True)\n",
    "\t\t\n",
    "\t\tself.embeddings = N.Embedding(num_embeddings= vocabSize, embedding_dim= dModel).to(device)\n",
    "\t\tself.posEmb = self.PositionalEmbedding(dModel= dModel)\n",
    "\t\tself.transformerEncoder = N.TransformerEncoder(encoder_layer= encoder, num_layers= nLayers)\n",
    "\t\tself.fc1 = N.Linear(in_features= dModel, out_features= dModel//2)\n",
    "\t\tself.fc2 = N.Linear(in_features= dModel//2, out_features= 2)\n",
    "\t\tself.fc3 = N.Linear(in_features= maxSeqLen * 2, out_features= 2)\n",
    "\n",
    "\tdef forward(self, input, padding_mask):\n",
    "\t\tembs = self.embeddings(input)\n",
    "\t\tbs, l, h = embs.shape\n",
    "\n",
    "\t\tseqIdx = torch.arange(l)\n",
    "\t\tposEmb = self.posEmb(seqIdx).reshape(1, l, h).expand(bs, l, h).to(device)\n",
    "\t\tembs = embs + posEmb\n",
    "\n",
    "\t\tcausalMask = torch.triu(torch.ones(l, l), 1).bool().to(device)\n",
    "\n",
    "\t\toutput = self.transformerEncoder(src= embs, mask= causalMask, \n",
    "\t\t\t\t\t\t\t\t   \t\t src_key_padding_mask= padding_mask)\n",
    "\n",
    "\t\tif(output.isnan().any()):\n",
    "\t\t\traise ValueError\n",
    "\t\t\n",
    "\t\toutput = self.fc1(output)\n",
    "\t\toutput = self.fc2(output).flatten(1,2)\n",
    "\t\toutput = self.fc3(output)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fbe20",
   "metadata": {},
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder(dModel, tokenizer.model_max_length, nLayers, nHeads, ffDim, vocabSize)\n",
    "model.to(device)\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[\n",
    "        torch.zeros((batchSize, tokenizer.model_max_length), dtype= torch.long).to(device),  # input\n",
    "        torch.zeros((batchSize, tokenizer.model_max_length), dtype= torch.bool).to(device)   # padding_mask\n",
    "    ],\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "    row_settings=['var_names'],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe0537",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "loss_fn = N.BCEWithLogitsLoss()\n",
    "mla = MultilabelAccuracy()\n",
    "\n",
    "epochSnapshot: list[dict] = []\n",
    "sep_token = torch.fill(torch.ones(batchSize, 1), SEP_TOKEN_ID).to(device)\n",
    "padding_mask_sep_token = torch.fill(torch.ones(batchSize, 1), False).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\tclear_output(wait= True)\n",
    "\n",
    "\tprint(f\"Starting epoch {epoch + 1} of {epochs}\\n~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\tepochLoss = 0\n",
    "\tepochAcc = 0\n",
    "\n",
    "\tfor idx, (file1_text, file2_text, label_tensor) in enumerate(modTrainDataloader):\n",
    "\t\tcurrBatchLoss = 0\n",
    "\t\tcurrBatchAccuracy = 0\n",
    "\t\t\n",
    "\t\tlabel_tensor = label_tensor.cuda()\n",
    "\t\t\n",
    "\t\ttokens1 = tokenizer(file1_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttokens2 = tokenizer(file2_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttoken_ids1 = tokens1['input_ids'].to(device)\n",
    "\t\ttoken_ids2 = tokens2['input_ids'].to(device)\n",
    "\t\t\n",
    "\t\ttarget_ids1 = torch.cat((token_ids1[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\t\ttarget_ids2 = torch.cat((token_ids2[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\t\t\n",
    "\t\ttarget_ids = torch.cat([target_ids1, sep_token, target_ids2], 1).long()\n",
    "\n",
    "\t\tpadding_mask1 = (~(tokens1['attention_mask'].bool())).to(device)\n",
    "\t\tpadding_mask2 = (~(tokens2['attention_mask'].bool())).to(device)\n",
    "\n",
    "\t\tpadding_mask = torch.cat([padding_mask1, padding_mask_sep_token, \n",
    "\t\t\t\t\t\t\tpadding_mask2], 1)\n",
    "\t\t\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\tgc.collect()\n",
    "\n",
    "\t\tpred = model(target_ids, padding_mask)\n",
    "\t\tloss = loss_fn(pred, label_tensor)\n",
    "\t\t\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\tpred_binary = torch.zeros_like(pred)\n",
    "\t\tpred_binary[torch.arange(pred.size(0)), pred.argmax(dim=1)] = 1\n",
    "\t\t\n",
    "\t\tmla.update(pred_binary, label_tensor)\n",
    "\t\tcurrBatchLoss = loss.detach().item()\n",
    "\t\tcurrBatchAccuracy += (mla.compute() * 100)\n",
    "\n",
    "\t\tepochLoss += currBatchLoss\n",
    "\t\tepochAcc += currBatchAccuracy\n",
    "\n",
    "\t\tif(idx % 5 == 0):\n",
    "\t\t\tprint(f\"Batch: {idx}; Curr batch loss: {currBatchLoss: 0.5f}; Curr batch acc.: {currBatchAccuracy:0.2f}%\")\n",
    "\t\t\n",
    "\tepochLoss /= len(modTrainDataloader)\n",
    "\tepochAcc /= len(modTrainDataloader)\n",
    "\t\n",
    "\tepochSnapshot.append({\n",
    "\t\t'epoch': f\"{epoch}\",\n",
    "\t\t'train_loss': f\"{epochLoss:0.7f}\",\n",
    "\t\t'train_acc': f\"{epochAcc:0.5f}\",\n",
    "\t})\n",
    "\n",
    "\tprint(f\"Avg train loss: {epochLoss:0.5f}; Avg train acc.: {epochAcc:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241c28a",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "testingPreds: list[dict] = []\n",
    "batchSize = 4 \n",
    "# setting batch size to 4 to ensure no samples are skipped\n",
    "# and 1068 (number of test files) is divisible by 4\n",
    "sep_token = torch.fill(torch.ones(batchSize, 1), SEP_TOKEN_ID).to(device)\n",
    "padding_mask_sep_token = torch.fill(torch.ones(batchSize, 1), False).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "\tfor idx, (articleId, file1_text, file2_text) in enumerate(modTestDataLoader):\n",
    "\n",
    "\t\ttokens1 = tokenizer(file1_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttokens2 = tokenizer(file2_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttoken_ids1 = tokens1['input_ids'].to(device)\n",
    "\t\ttoken_ids2 = tokens2['input_ids'].to(device)\n",
    "\t\t\n",
    "\t\ttarget_ids1 = torch.cat((token_ids1[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\t\ttarget_ids2 = torch.cat((token_ids2[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\t\t\n",
    "\t\ttarget_ids = torch.cat([target_ids1, sep_token, target_ids2], 1).long()\n",
    "\n",
    "\t\tpadding_mask1 = (~(tokens1['attention_mask'].bool())).to(device)\n",
    "\t\tpadding_mask2 = (~(tokens2['attention_mask'].bool())).to(device)\n",
    "\n",
    "\t\tpadding_mask = torch.cat([padding_mask1, padding_mask_sep_token, \n",
    "\t\t\t\t\t\t\tpadding_mask2], 1)\n",
    "\t\t\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\tgc.collect()\n",
    "\n",
    "\t\tpred = model(target_ids, padding_mask)\n",
    "\t\tpredFileIds = pred.argmax(dim= 1) + 1\n",
    "\n",
    "\t\tfor i in range(batchSize):\n",
    "\t\t\ttestingPreds.append({\n",
    "\t\t\t\t'id': articleId[i].item(),\n",
    "\t\t\t\t'real_text_id': predFileIds[i].item()\n",
    "\t\t\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5eef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(testingPreds)\n",
    "df.to_csv(OUT_FILE_M2, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f274d",
   "metadata": {},
   "source": [
    "## Approach 3: Dual encoder approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69bbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_AND_LABELS_M3 = \"./kaggle/input/fake-or-real-the-impostor-hunt/data/train_files_and_ground_truth-m2.json\"\n",
    "TEST_FILES_M3 = \"./kaggle/input/fake-or-real-the-impostor-hunt/data/test_files_m2.json\"\n",
    "OUT_FILE_M3 = \"./kaggle/working/fake-or-real-the-impostor-hunt/submission-method-3.csv\"\n",
    "\n",
    "maxSeqLen = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933733a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c433dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTrainDataset(Dataset):\n",
    "\tdef __init__(self, path):\n",
    "\t\tsuper(ModifiedTrainDataset, self).__init__()\n",
    "\t\tself.__filesAndLabels__: pd.DataFrame = pd.read_json(path)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.__filesAndLabels__.shape[0]\n",
    "\t\n",
    "\tdef __getfiletext__(self, filePath: str):\n",
    "\t\tinputText = \"\"\n",
    "\t\twith open(filePath, 'r') as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tinputText += line + \"\\n\"\n",
    "\n",
    "\t\treturn inputText\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t_, files = self.__filesAndLabels__.iloc[index]\n",
    "\t\tfiles = sorted(files, key= lambda x: x['fileId'])\n",
    "\n",
    "\t\tfile1_path = files[0]['filePath']\n",
    "\t\tfile1_label = int(files[0]['label'])\n",
    "\t\tfile1_text = self.__getfiletext__(file1_path)\n",
    "\n",
    "\t\tfile2_path = files[1]['filePath']\n",
    "\t\tfile2_label = int(files[1]['label'])\n",
    "\t\tfile2_text = self.__getfiletext__(file2_path)\n",
    "\n",
    "\t\tlabel_tensor = torch.tensor([file1_label, file2_label], dtype= torch.float)\n",
    "\n",
    "\t\treturn file1_text, file2_text, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86bad278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTestDataset(Dataset):\n",
    "\tdef __init__(self, path):\n",
    "\t\tsuper(ModifiedTestDataset, self).__init__()\n",
    "\t\tself.__articleAndFiles__: pd.DataFrame = pd.read_json(path)\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.__articleAndFiles__.shape[0]\n",
    "\t\n",
    "\tdef __getfiletext__(self, filePath: str):\n",
    "\t\tinputText = \"\"\n",
    "\t\twith open(filePath, 'r') as f:\n",
    "\t\t\tfor line in f.readlines():\n",
    "\t\t\t\tinputText += line + \"\\n\"\n",
    "\n",
    "\t\treturn inputText\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tarticleId, files = self.__articleAndFiles__.iloc[index]\n",
    "\t\tfiles = sorted(files, key= lambda x: x['fileId'])\n",
    "\n",
    "\t\tfile1_path = files[0]['filePath']\n",
    "\t\tfile1_text = self.__getfiletext__(file1_path)\n",
    "\n",
    "\t\tfile2_path = files[1]['filePath']\n",
    "\t\tfile2_text = self.__getfiletext__(file2_path)\n",
    "\n",
    "\t\treturn articleId.item(), file1_text, file2_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35589fdb",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1498901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiedTrainData = ModifiedTrainDataset(FILES_AND_LABELS_M3)\n",
    "modifiedTestData = ModifiedTestDataset(TEST_FILES_M3)\n",
    "\n",
    "modTrainDataloader = DataLoader(modifiedTrainData, batch_size= batchSize,\n",
    "\t\t\t\t\t\t\t shuffle= True, drop_last= True)\n",
    "modTestDataLoader = DataLoader(modifiedTestData, batch_size= 4,\n",
    "\t\t\t\t\t\t\tshuffle= True, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cbf13",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc696bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(N.Module):\n",
    "\tclass PositionalEmbedding(N.Module):\n",
    "\t\tdef __init__(self, dModel):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.dModel = dModel\n",
    "\n",
    "\t\tdef forward(self, input):\n",
    "\t\t\temb = math.log(10000) / (self.dModel // 2 - 1)\n",
    "\t\t\temb = torch.exp(torch.arange(self.dModel // 2) * -emb)\n",
    "\t\t\temb = input[:, None] * emb[None, :]\n",
    "\t\t\temb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\t\t\treturn emb\n",
    "\n",
    "\tdef __init__(self, dModel, maxSeqLen, nLayers, nHeads, ffDim, vocabSize, dropout= 0.1):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tencoder = N.TransformerEncoderLayer(d_model= dModel, nhead= nHeads,\n",
    "\t\t\t\t\t\t\t\t\t  \t\tdim_feedforward= ffDim, dropout= dropout,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_first= True)\n",
    "\t\t\n",
    "\t\tself.embeddings = N.Embedding(num_embeddings= vocabSize, embedding_dim= dModel).to(device)\n",
    "\t\tself.posEmb = self.PositionalEmbedding(dModel= dModel)\n",
    "\t\tself.transformerEncoder = N.TransformerEncoder(encoder_layer= encoder, num_layers= nLayers)\n",
    "\t\tself.fc1 = N.Linear(in_features= dModel, out_features= dModel//2)\n",
    "\t\tself.fc2 = N.Linear(in_features= dModel//2, out_features= dModel//4)\n",
    "\t\tself.fc3 = N.Linear(in_features= (maxSeqLen * 2) * dModel//4, out_features= 2)\n",
    "\n",
    "\tdef forward(self, input1, padding_mask1, input2, padding_mask2):\n",
    "\t\tembs1 = self.embeddings(input1)\n",
    "\t\tbs1, l1, h1 = embs1.shape\n",
    "\n",
    "\t\tseqIdx1 = torch.arange(l1)\n",
    "\t\tposEmb1 = self.posEmb(seqIdx1).reshape(1, l1, h1).expand(bs1, l1, h1).to(device)\n",
    "\t\tembs1 = embs1 + posEmb1\n",
    "\n",
    "\t\tcausalMask1 = torch.triu(torch.ones(l1, l1), 1).bool().to(device)\n",
    "\n",
    "\t\toutput1 = self.transformerEncoder(src= embs1, mask= causalMask1, \n",
    "\t\t\t\t\t\t\t\t   \t\t src_key_padding_mask= padding_mask1)\n",
    "\t\t\n",
    "\t\tembs2 = self.embeddings(input2)\n",
    "\t\tbs2, l2, h2 = embs2.shape\n",
    "\n",
    "\t\tseqIdx2 = torch.arange(l2)\n",
    "\t\tposEmb2 = self.posEmb(seqIdx2).reshape(1, l2, h2).expand(bs2, l2, h2).to(device)\n",
    "\t\tembs2 = embs2 + posEmb2\n",
    "\n",
    "\t\tcausalMask2 = torch.triu(torch.ones(l2, l2), 1).bool().to(device)\n",
    "\n",
    "\t\toutput2 = self.transformerEncoder(src= embs2, mask= causalMask2, \n",
    "\t\t\t\t\t\t\t\t   \t\t src_key_padding_mask= padding_mask2)\n",
    "\t\t\n",
    "\t\toutput = torch.cat([output1, output2], dim= 1)\n",
    "\t\toutput = self.fc1(output)\n",
    "\t\toutput = self.fc2(output).flatten(1,2)\n",
    "\t\toutput = self.fc3(output)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8cde9",
   "metadata": {},
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84c15a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape               Output Shape              Param #                   Trainable\n",
       "=================================================================================================================================================\n",
       "Encoder (Encoder)                             [8, 512]                  [8, 2]                    --                        True\n",
       "├─Embedding (embeddings)                      [8, 512]                  [8, 512, 1024]            31,254,528                True\n",
       "├─PositionalEmbedding (posEmb)                [512]                     [512, 1024]               --                        --\n",
       "├─TransformerEncoder (transformerEncoder)     --                        [8, 512, 1024]            --                        True\n",
       "│    └─ModuleList (layers)                    --                        --                        (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (0)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (1)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (2)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (3)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (4)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (5)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (6)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "│    │    └─TransformerEncoderLayer (7)       [8, 512, 1024]            [8, 512, 1024]            6,301,696                 True\n",
       "├─Embedding (embeddings)                      [8, 512]                  [8, 512, 1024]            (recursive)               True\n",
       "├─PositionalEmbedding (posEmb)                [512]                     [512, 1024]               --                        --\n",
       "├─TransformerEncoder (transformerEncoder)     --                        [8, 512, 1024]            (recursive)               True\n",
       "│    └─ModuleList (layers)                    --                        --                        (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (0)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (1)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (2)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (3)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (4)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (5)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (6)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "│    │    └─TransformerEncoderLayer (7)       [8, 512, 1024]            [8, 512, 1024]            (recursive)               True\n",
       "├─Linear (fc1)                                [8, 1024, 1024]           [8, 1024, 512]            524,800                   True\n",
       "├─Linear (fc2)                                [8, 1024, 512]            [8, 1024, 256]            131,328                   True\n",
       "├─Linear (fc3)                                [8, 262144]               [8, 2]                    524,290                   True\n",
       "=================================================================================================================================================\n",
       "Total params: 82,848,514\n",
       "Trainable params: 82,848,514\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 509.52\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 117.44\n",
       "Params size (MB): 129.74\n",
       "Estimated Total Size (MB): 247.25\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder(dModel, tokenizer.model_max_length, nLayers, nHeads, ffDim, vocabSize)\n",
    "model.to(device)\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[\n",
    "        torch.zeros((batchSize, tokenizer.model_max_length), dtype= torch.long).to(device),  # input\n",
    "        torch.zeros((batchSize, tokenizer.model_max_length), dtype= torch.bool).to(device),   # padding_mask\n",
    "\t\ttorch.zeros((batchSize, tokenizer.model_max_length), dtype= torch.long).to(device),  # input\n",
    "        torch.zeros((batchSize, tokenizer.model_max_length), dtype= torch.bool).to(device)   # padding_mask\n",
    "    ],\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "    row_settings=['var_names'],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad70b5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86eb06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 10 of 10\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Batch: 0; Curr batch loss:  0.00047; Curr batch acc.: 89.38%\n",
      "Batch: 5; Curr batch loss:  0.00085; Curr batch acc.: 89.88%\n",
      "Batch: 10; Curr batch loss:  0.00000; Curr batch acc.: 90.34%\n",
      "Avg train loss: 0.00060; Avg train acc.: 89.87177\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr= lr)\n",
    "loss_fn = N.BCEWithLogitsLoss()\n",
    "mla = MultilabelAccuracy()\n",
    "\n",
    "epochSnapshot: list[dict] = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\tclear_output(wait= True)\n",
    "\n",
    "\tprint(f\"Starting epoch {epoch + 1} of {epochs}\\n~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\tepochLoss = 0\n",
    "\tepochAcc = 0\n",
    "\n",
    "\tfor idx, (file1_text, file2_text, label_tensor) in enumerate(modTrainDataloader):\n",
    "\t\tcurrBatchLoss = 0\n",
    "\t\tcurrBatchAccuracy = 0\n",
    "\t\t\n",
    "\t\tlabel_tensor = label_tensor.cuda()\n",
    "\t\t\n",
    "\t\ttokens1 = tokenizer(file1_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttokens2 = tokenizer(file2_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttoken_ids1 = tokens1['input_ids'].to(device)\n",
    "\t\ttoken_ids2 = tokens2['input_ids'].to(device)\n",
    "\t\t\n",
    "\t\ttarget_ids1 = torch.cat((token_ids1[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\t\ttarget_ids2 = torch.cat((token_ids2[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\n",
    "\t\tpadding_mask1 = (~(tokens1['attention_mask'].bool())).to(device)\n",
    "\t\tpadding_mask2 = (~(tokens2['attention_mask'].bool())).to(device)\n",
    "\t\t\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\tgc.collect()\n",
    "\n",
    "\t\tpred = model(target_ids1, padding_mask1, target_ids2, padding_mask2)\n",
    "\t\tloss = loss_fn(pred, label_tensor)\n",
    "\t\t\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\tpred_binary = torch.zeros_like(pred)\n",
    "\t\tpred_binary[torch.arange(pred.size(0)), pred.argmax(dim=1)] = 1\n",
    "\t\t\n",
    "\t\tmla.update(pred_binary, label_tensor)\n",
    "\t\tcurrBatchLoss = loss.detach().item()\n",
    "\t\tcurrBatchAccuracy += (mla.compute() * 100)\n",
    "\n",
    "\t\tepochLoss += currBatchLoss\n",
    "\t\tepochAcc += currBatchAccuracy\n",
    "\n",
    "\t\tif(idx % 5 == 0):\n",
    "\t\t\tprint(f\"Batch: {idx}; Curr batch loss: {currBatchLoss: 0.5f}; Curr batch acc.: {currBatchAccuracy:0.2f}%\")\n",
    "\t\t\n",
    "\tepochLoss /= len(modTrainDataloader)\n",
    "\tepochAcc /= len(modTrainDataloader)\n",
    "\t\n",
    "\tepochSnapshot.append({\n",
    "\t\t'epoch': f\"{epoch}\",\n",
    "\t\t'train_loss': f\"{epochLoss:0.7f}\",\n",
    "\t\t'train_acc': f\"{epochAcc:0.5f}\",\n",
    "\t})\n",
    "\n",
    "\tprint(f\"Avg train loss: {epochLoss:0.5f}; Avg train acc.: {epochAcc:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6251bb1",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e810e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "testingPreds: list[dict] = []\n",
    "batchSize = 4 \n",
    "# setting batch size to 4 to ensure no samples are skipped\n",
    "# and 1068 (number of test files) is divisible by 4\n",
    "\n",
    "with torch.inference_mode():\n",
    "\tfor idx, (articleId, file1_text, file2_text) in enumerate(modTestDataLoader):\n",
    "\n",
    "\t\ttokens1 = tokenizer(file1_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttokens2 = tokenizer(file2_text, padding= \"max_length\", \n",
    "\t\t\t\t\t \t   truncation= True, max_length= maxSeqLen,\n",
    "\t\t\t\t\t\t   return_tensors= \"pt\")\n",
    "\t\t\n",
    "\t\ttoken_ids1 = tokens1['input_ids'].to(device)\n",
    "\t\ttoken_ids2 = tokens2['input_ids'].to(device)\n",
    "\t\t\n",
    "\t\ttarget_ids1 = torch.cat((token_ids1[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\t\ttarget_ids2 = torch.cat((token_ids2[:, 1:], \n",
    "\t\t\t\t\t\t\t\ttorch.zeros(batchSize, 1, device=device).long()), 1)\n",
    "\n",
    "\t\tpadding_mask1 = (~(tokens1['attention_mask'].bool())).to(device)\n",
    "\t\tpadding_mask2 = (~(tokens2['attention_mask'].bool())).to(device)\n",
    "\t\t\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\tgc.collect()\n",
    "\n",
    "\t\tpred = model(target_ids1, padding_mask1, target_ids2, padding_mask2)\n",
    "\t\tpredFileIds = pred.argmax(dim= 1) + 1\n",
    "\n",
    "\t\tfor i in range(batchSize):\n",
    "\t\t\ttestingPreds.append({\n",
    "\t\t\t\t'id': articleId[i].item(),\n",
    "\t\t\t\t'real_text_id': predFileIds[i].item()\n",
    "\t\t\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee7e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(testingPreds)\n",
    "df.to_csv(OUT_FILE_M3, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f8e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
